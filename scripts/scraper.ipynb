{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -e git+https://github.com/gauravmm/jupyter-testing.git#egg=jupyter-testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The scraper notebook: \n",
    "\n",
    "This notebook contains cells to scrape the needed data. There are two major tasks in this notebook. \n",
    "1. For each of the four genres, 1250 books and their attributes (e.g. book title, book author) are collected. \n",
    "2. For the fiction genre, as many reviews are collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup library imports\n",
    "# For now the requests library will not be used since we are collecting the data manually\n",
    "# import requests\n",
    "\n",
    "import os \n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from testing.testing import test\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Collecting book attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(file_path): \n",
    "    \"\"\"\n",
    "    Retrieve ALL the html pages on goodreads for given genre.\n",
    "\n",
    "    Returns:\n",
    "        roots (list): list of bs4 objects for html file\n",
    "    \"\"\"\n",
    "    lst_html = list()\n",
    "    roots = list()\n",
    "\n",
    "    \n",
    "    for filename in sorted(os.listdir(file_path)):\n",
    "        with open(os.path.join(file_path, filename)) as f:\n",
    "            content = f.read()\n",
    "            lst_html.append(content)\n",
    "\n",
    "    for html_page in lst_html: \n",
    "        # response.text (string): String of HTML corresponding to a page of 50 books\n",
    "        root = BeautifulSoup(html_page, 'html.parser')        \n",
    "        roots.append(root)\n",
    "\n",
    "    return roots\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_page(roots):\n",
    "    \"\"\"\n",
    "    Parse the reviews on each of the 25 pages.\n",
    "    \n",
    "    Args:\n",
    "        book_attributes (list): book_title, author_name, ratings, num_of_ratings, date_published\n",
    "\n",
    "    Returns:\n",
    "        book_attributes (list) : \n",
    "        - book_url, book_title, author_name, ratings, num_of_ratings, date_published\n",
    "    \"\"\"\n",
    "    \n",
    "    book_id, book_url, book_title, author_name, ratings, num_of_ratings, date_published, book_shelved, book_genre = list(), list(), list(), list(), list(), list(), list(), list(), list()\n",
    "    book_attributes = list()\n",
    "\n",
    "    for root in roots:\n",
    "        book_link_prefix = \"https://www.goodreads.com\"\n",
    "        book_url_page = [x['href'] for x in root.find_all(\"a\", class_=\"bookTitle\")]\n",
    "        \n",
    "        book_id_page = [book_link.split(\"/book/show/\")[1].split(\".\")[0] for book_link in book_url_page]\n",
    "        book_id.extend(book_id_page)\n",
    "\n",
    "        book_url_page = [book_link_prefix+book_link for book_link in book_url_page]\n",
    "        book_url.extend(book_url_page)\n",
    "        \n",
    "        book_title_page = [x.get_text() for x in root.find_all(\"a\", class_=\"bookTitle\")]\n",
    "        book_title.extend(book_title_page)\n",
    "\n",
    "        author_name_page = [x.get_text() for x in root.find_all(\"a\", class_=\"authorName\")]\n",
    "        author_name.extend(author_name_page)\n",
    "\n",
    "        ratings_data = []\n",
    "        shevles_genre_data = []\n",
    "\n",
    "        for div in root.find_all(\"div\", class_=\"left\"):\n",
    "            start = 'shelved'\n",
    "            end = 'avg rating'\n",
    "            s = div.get_text()\n",
    "            shevles_genre_data = s[s.find(start)+len(start):s.rfind(end)]\n",
    "\n",
    "            keyword = \" times as \"\n",
    "            before_keyword, keyword, after_keyword = shevles_genre_data.partition(keyword)\n",
    "            book_shelved.append(int(before_keyword))\n",
    "            book_genre.append(after_keyword.split()[0][:-1])\n",
    "            \n",
    "\n",
    "        for div in root.find_all(\"div\", class_=\"left\"):\n",
    "            for span in div.find_all('span', {'class' : 'greyText smallText'}):\n",
    "                ratings_data.append(span.get_text())\n",
    "        \n",
    "        for elem in ratings_data: \n",
    "\n",
    "            ratings.append(elem.split()[2])\n",
    "            num_of_ratings.append(elem.split()[4])\n",
    "            \n",
    "            # If date published is not given pass in nan value\n",
    "            if len(elem.split()) < 9: \n",
    "                date_published.append(np.nan)\n",
    "            else: \n",
    "                date_published.append(elem.split()[8])\n",
    "\n",
    "    book_attributes = [book_id, book_url, book_title, author_name, ratings, num_of_ratings, date_published, book_shelved, book_genre]\n",
    "    \n",
    "    return book_attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(book_attributes):\n",
    "    \"\"\"\n",
    "    Create a dataframe\n",
    "    \n",
    "    Args:\n",
    "        book_attributes (list): book_title, author_name, ratings, num_of_ratings, date_published\n",
    "        \n",
    "    Returns:\n",
    "        df (pd.DataFrame) : \n",
    "        - Columns: book_title, author_name, ratings, num_of_ratings, date_published\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {'book_id': book_attributes[0],\n",
    "        'book_url': book_attributes[1],\n",
    "        'book_title': book_attributes[2],\n",
    "        'author_name': book_attributes[3],\n",
    "        'ratings': book_attributes[4],\n",
    "        'num_of_ratings': book_attributes[5],\n",
    "        'date_published': book_attributes[6],\n",
    "        'book_shelved': book_attributes[7],\n",
    "        'book_genre': book_attributes[8]\n",
    "        })\n",
    "    \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = ['../HTML/Fiction', '../HTML/Science', '../HTML/Religion', '../HTML/Crime']\n",
    "for file_path in file_paths: \n",
    "    roots = get_html(file_path)\n",
    "    book_attributes = parse_page(roots)\n",
    "    df = create_dataframe(book_attributes)\n",
    "    filename = 'goodreads_' + file_path.split('/')[-1] + '.csv'\n",
    "    df.to_csv(filename, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Collecting reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F = open(\"IDS.txt\", \"w\")\n",
    "# for line in df[\"book_id\"]:\n",
    "#     F.write(line)\n",
    "#     F.write(\"\\n\")\n",
    "# F.close()\n",
    "\n",
    "# !mkdir book_reviews_folder\n",
    "\n",
    "# !python get_reviews.py --book_ids_path IDS.txt \\\n",
    "# --output_directory_path book_reviews_folder --sort_order default --browser chrome \n",
    "# reviews_df = pd.read_json('book_reviews_folder/all_reviews.json')\n",
    "# reviews_df.to_csv(\"review.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7d9279c439293cd56693e7c05115c53cbb1633ee63d7052d68a345e17a860490"
  },
  "kernelspec": {
   "display_name": "csx433env",
   "language": "python",
   "name": "csx433env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
